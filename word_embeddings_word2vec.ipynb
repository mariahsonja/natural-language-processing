{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings (word2vec)\n",
    "\n",
    "**Word embeddings** is **vectors** representations of a word.\n",
    "\n",
    "**Word2Vec** is one of the most popular technique to learn word embeddings using shallow neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /Users/msonjap/anaconda3/lib/python3.7/site-packages (3.8.3)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /Users/msonjap/anaconda3/lib/python3.7/site-packages (from gensim) (1.18.5)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/msonjap/anaconda3/lib/python3.7/site-packages (from gensim) (4.0.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/msonjap/anaconda3/lib/python3.7/site-packages (from gensim) (1.5.2)\n",
      "Requirement already satisfied: six>=1.5.0 in /Users/msonjap/anaconda3/lib/python3.7/site-packages (from gensim) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "# installing packages\n",
    "! pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/msonjap/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing packages\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import string\n",
    "import gensim\n",
    "import bokeh.io\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.manifold import TSNE\n",
    "from bokeh.io import output_notebook, output_file\n",
    "from bokeh.plotting import show, figure\n",
    "from bokeh.resources import INLINE\n",
    "\n",
    "%matplotlib inline\n",
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get corpus of Project Gutenberg books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting public domain books\n",
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# showing the public domain document names that are in the corpus\n",
    "gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2621613"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting total number of words in the corpus\n",
    "len(gutenberg.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returning sentences tokenized\n",
    "gberg_sents = gutenberg.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VOLUME', 'I']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# showing tokenized sents\n",
    "gberg_sents[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing (only) capitalization and punctuation from the corpus\n",
    "lower_sents = []\n",
    "for sentence in gberg_sents:\n",
    "    lower_sents.append([word.lower() for word in sentence if word.lower()\n",
    "                        not in list(string.punctuation)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using word2vec model\n",
    "model = Word2Vec(sentences=lower_sents, size=64,\n",
    "                 sg=1, window=10, iter=5,\n",
    "                 min_count=10, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.22279131,  0.29291525, -0.10614601,  0.04117888,  0.00455246,\n",
       "       -0.18285047, -0.1974895 ,  0.1709207 , -0.00104911,  0.27018604,\n",
       "       -0.1106019 ,  0.33000317, -0.2310855 ,  0.12785162, -0.14745755,\n",
       "       -0.48652875, -0.21596232, -0.5863022 , -0.03720129, -0.5991778 ,\n",
       "       -0.09051408,  0.01368778, -0.18260558, -0.05264063, -0.22875598,\n",
       "        0.05219515, -0.28427622,  0.32841358, -0.2831135 , -0.43946558,\n",
       "       -0.1576695 , -0.03575251, -0.11319762, -0.07995832, -0.32105467,\n",
       "        0.10908943, -0.45934325,  0.2865859 , -0.29813007,  0.03949017,\n",
       "       -0.55328125,  0.00140527, -0.02543302,  0.567693  ,  0.1200754 ,\n",
       "        0.43267724,  0.7768422 ,  0.42127737, -0.33191466,  0.29339343,\n",
       "       -0.16830774, -0.4158343 ,  0.11786711,  0.36775258,  0.12001032,\n",
       "       -0.01372659,  0.11720549,  0.38484454, -0.36438483, -0.13066961,\n",
       "       -0.3189706 , -0.18296607, -0.40573075,  0.28403422], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv[\"dog\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mother', 0.8260801434516907),\n",
       " ('brother', 0.7541067004203796),\n",
       " ('cousin', 0.7053717374801636),\n",
       " ('uncle', 0.6931771039962769)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting top n words most similar to a given word - using cosine similarity\n",
    "model.wv.most_similar('father', topn=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.58774924"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getting the similarity score between two words\n",
    "model.wv.similarity('father','dog')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the word similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reducing the 64 dimensions down to 2 in order to visualize\n",
    "tsne = TSNE(n_components=2, n_iter=1000)\n",
    "X_2d = tsne.fit_transform(model.wv[model.wv.vocab])\n",
    "coords_df = pd.DataFrame(X_2d, columns=['x','y'])\n",
    "coords_df['token'] = model.wv.vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2d scatterplot static visualization\n",
    "_ = coords_df.plot.scatter('x','y', figsize=(12,12),\n",
    "                    marker='.', s=10, alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a Bokeh interactive visualization\n",
    "# output_notebook()\n",
    "bokeh.io.output_notebook(INLINE)\n",
    "subset_df = coords_df.sample(n=5000)\n",
    "p = figure(plot_width=800, plot_height=800)\n",
    "_ = p.text(x=subset_df.x, y=subset_df.y, text=subset_df.token)\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
